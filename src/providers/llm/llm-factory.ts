import { ConfigManager } from "@src/utils/config/config-manager.ts";
import {
  LLMProvider,
  LLMProviderType,
  LLMProviderTypeMap,
} from "@src/providers/interfaces/llm.interface.ts";
import { OpenAICompatibleLLM } from "@src/providers/llm/openai-compatible-llm.ts";
import { XunfeiLLM } from "@src/providers/llm/xunfei-llm.ts";

/**
 * è§£æLLMæä¾›è€…é…ç½?
 * æ”¯æŒä¸¤ç§æ ¼å¼:
 * 1. ç®€å•æ ¼å¼? "PROVIDER" - ä»…æŒ‡å®šæä¾›è€…ç±»å?
 * 2. æ‰©å±•æ ¼å¼: "PROVIDER:model" - æŒ‡å®šæä¾›è€…ç±»å‹å’Œæ¨¡å‹
 */
interface ParsedLLMConfig {
  providerType: LLMProviderType;
  model?: string;
}

/**
 * LLMå·¥å‚ç±»ï¼Œç”¨äºåˆ›å»ºå’Œç®¡ç†ä¸åŒçš„LLMæä¾›è€…å®ä¾?
 */
export class LLMFactory {
  private static instance: LLMFactory;
  private providers: Map<string, LLMProvider> = new Map();
  private configManager: ConfigManager;

  private constructor() {
    this.configManager = ConfigManager.getInstance();
  }

  /**
   * è·å–LLMå·¥å‚å•ä¾‹
   */
  public static getInstance(): LLMFactory {
    if (!LLMFactory.instance) {
      LLMFactory.instance = new LLMFactory();
    }
    return LLMFactory.instance;
  }

  /**
   * è§£æLLMæä¾›è€…é…ç½®å­—ç¬¦ä¸²
   * @param config é…ç½®å­—ç¬¦ä¸²ï¼Œæ ¼å¼ä¸?"PROVIDER" æˆ?"PROVIDER:model"
   * @returns è§£æåçš„é…ç½®å¯¹è±¡
   */
  private parseLLMConfig(config: string): ParsedLLMConfig {
    const parts = config.split(":");
    const providerType = parts[0] as LLMProviderType;
    const model = parts.length > 1 ? parts[1] : undefined;

    return { providerType, model };
  }

  /**
   * è·å–æä¾›è€…ç¼“å­˜é”®
   * å¯¹äºæŒ‡å®šäº†æ¨¡å‹çš„æä¾›è€…ï¼Œä½¿ç”¨ "PROVIDER:model" ä½œä¸ºé”?
   * @param config è§£æåçš„é…ç½®å¯¹è±¡
   * @returns ç¼“å­˜é”?
   */
  private getProviderCacheKey(config: ParsedLLMConfig): string {
    return config.model
      ? `${config.providerType}:${config.model}`
      : config.providerType;
  }

  /**
   * è·å–æŒ‡å®šç±»å‹çš„LLMæä¾›è€?
   * @param typeOrConfig LLMæä¾›è€…é…ç½®å­—ç¬¦ä¸²
   * @param needRefresh æ˜¯å¦éœ€è¦åˆ·æ–°æä¾›è€…é…ç½?
   * @returns LLMæä¾›è€…å®ä¾?
   */
  public async getLLMProvider<T extends ParsedLLMConfig>(
    typeOrConfig: T | string,
    needRefresh: boolean = true,
  ): Promise<LLMProviderTypeMap[T["providerType"]]> {
    // è§£æé…ç½®
    const config = typeof typeOrConfig === "string"
      ? this.parseLLMConfig(typeOrConfig)
      : typeOrConfig;

    type ProviderType = LLMProviderTypeMap[T["providerType"]];

    // è·å–ç¼“å­˜é”?
    const cacheKey = this.getProviderCacheKey(config);

    // å¦‚æœå·²ç»åˆ›å»ºè¿‡è¯¥ç±»å‹çš„æä¾›è€…ï¼Œä¸”ä¸éœ€è¦åˆ·æ–°ï¼Œç›´æ¥è¿”å›
    if (this.providers.has(cacheKey) && !needRefresh) {
      return this.providers.get(cacheKey)! as ProviderType;
    }

    // å¦‚æœéœ€è¦åˆ·æ–°ä¸”æä¾›è€…å­˜åœ¨ï¼Œå…ˆåˆ·æ–°é…ç½?
    if (needRefresh && this.providers.has(cacheKey)) {
      await this.providers.get(cacheKey)!.refresh();
      return this.providers.get(cacheKey)! as ProviderType;
    }

    // æ ¹æ®ç±»å‹åˆ›å»ºå¯¹åº”çš„LLMæä¾›è€?
    let provider: LLMProvider;

    switch (config.providerType) {
      case "OPENAI":
        provider = new OpenAICompatibleLLM("OPENAI_", undefined, config.model);
        break;
      case "DEEPSEEK":
        provider = new OpenAICompatibleLLM(
          "DEEPSEEK_",
          undefined,
          config.model,
        );
        break;
      case "XUNFEI":
        // è®¯é£ä¸æ”¯æŒæŒ‡å®šæ¨¡å?
        if (config.model) {
          console.warn(
            `è­¦å‘Š: è®¯é£APIä¸æ”¯æŒæŒ‡å®šæ¨¡å‹ï¼Œå°†å¿½ç•¥æ¨¡å‹è®¾ç½? ${config.model}`,
          );
        }
        provider = new XunfeiLLM();
        break;
      case "QWEN":
        provider = new OpenAICompatibleLLM("QWEN_", undefined, config.model);
        break;
      case "CUSTOM":
        provider = new OpenAICompatibleLLM(
          "CUSTOM_LLM_",
          undefined,
          config.model,
        );
        break;
      default:
        throw new Error(`ä¸æ”¯æŒçš„LLMæä¾›è€…ç±»å? ${config.providerType}`);
    }

    // åˆå§‹åŒ–æä¾›è€?
    try {
      await provider.initialize();
      this.providers.set(cacheKey, provider);
      return provider as ProviderType;
    } catch (error) {
      console.error(`åˆå§‹åŒ–LLMæä¾›è€…å¤±è´?[${cacheKey}]:`, error);
      throw new Error(
        `æ— æ³•åˆå§‹åŒ–LLMæä¾›è€?[${cacheKey}]: ${(error as Error).message}`,
      );
    }
  }

  /**
   * åˆ·æ–°æ‰€æœ‰å·²åˆ›å»ºçš„LLMæä¾›è€…çš„é…ç½®
   */
  public async refreshAllProviders(): Promise<void> {
    const refreshPromises: Promise<void>[] = [];

    for (const [type, provider] of this.providers.entries()) {
      refreshPromises.push(
        provider.refresh().catch((error) => {
          console.error(`åˆ·æ–°LLMæä¾›è€…é…ç½®å¤±è´?[${type}]:`, error);
        }),
      );
    }

    await Promise.allSettled(refreshPromises);
  }

  /**
   * è·å–é»˜è®¤çš„LLMæä¾›è€?
   * ä»é…ç½®ä¸­è¯»å–DEFAULT_LLM_PROVIDERï¼Œå¦‚æœæœªé…ç½®åˆ™é»˜è®¤ä½¿ç”¨OpenAI
   */
  public async getDefaultProvider(): Promise<LLMProvider> {
    try {
      const defaultProviderConfig =
        await this.configManager.get("DEFAULT_LLM_PROVIDER") || "OPENAI";
      return this.getLLMProvider(defaultProviderConfig as string);
    } catch (error) {
      console.error("è·å–é»˜è®¤LLMæä¾›è€…å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨OpenAIä½œä¸ºå¤‡é€?", error);
      return this.getLLMProvider("OPENAI");
    }
  }
}
