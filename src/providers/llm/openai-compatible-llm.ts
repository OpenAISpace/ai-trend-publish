import { ConfigManager } from "@src/utils/config/config-manager.ts";
import { HttpClient } from "@src/utils/http/http-client.ts";
import {
  ChatCompletionOptions,
  ChatMessage,
  LLMProvider,
} from "@src/providers/interfaces/llm.interface.ts";

export class OpenAICompatibleLLM implements LLMProvider {
  private baseURL!: string;
  private token!: string;
  private defaultModel!: string;
  private availableModels: string[] = [];
  private httpClient: HttpClient;

  constructor(
    private configKeyPrefix: string = "",
    private configManager: ConfigManager = ConfigManager.getInstance(),
    private specifiedModel?: string,
  ) {
    this.httpClient = HttpClient.getInstance();
  }

  async initialize(): Promise<void> {
    await this.refresh();
  }

  async refresh(): Promise<void> {
    this.baseURL = await this.configManager.get(
      `${this.configKeyPrefix}BASE_URL`,
    );
    this.token = await this.configManager.get(`${this.configKeyPrefix}API_KEY`);

    // è·å–æ¨¡å‹é…ç½®ï¼Œæ”¯æŒå¤šæ¨¡å‹æ ¼å¼ "model1|model2|model3"
    const modelConfig =
      await this.configManager.get(`${this.configKeyPrefix}MODEL`) ||
      "gpt-3.5-turbo";
    this.availableModels = (modelConfig as string).split("|").map((
      model: string,
    ) => model.trim());

    // å¦‚æœæŒ‡å®šäº†ç‰¹å®šæ¨¡å‹ï¼Œä½¿ç”¨æŒ‡å®šçš„æ¨¡å‹ï¼Œå¦åˆ™ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨æ¨¡å?
    this.defaultModel = this.specifiedModel || this.availableModels[0];

    if (!this.baseURL) {
      throw new Error(`${this.configKeyPrefix}BASE_URL is not set`);
    }
    if (!this.token) {
      throw new Error(`${this.configKeyPrefix}API_KEY is not set`);
    }

    // æ£€æŸ¥APIæœåŠ¡æ˜¯å¦å¯ç”¨
    const isHealthy = await this.httpClient.healthCheck(this.baseURL);
    if (!isHealthy) {
      console.warn(
        `è­¦å‘Š: LLMæœåŠ¡ ${this.baseURL} å¥åº·æ£€æŸ¥å¤±è´¥ï¼Œå¯èƒ½æ— æ³•æ­£å¸¸è®¿é—®`,
      );
    }
  }

  /**
   * è®¾ç½®ä½¿ç”¨çš„æ¨¡å?
   * @param model æ¨¡å‹åç§°
   */
  public setModel(model: string): void {
    if (this.availableModels.includes(model)) {
      this.defaultModel = model;
    } else {
      console.warn(
        `è­¦å‘Š: æ¨¡å‹ ${model} ä¸åœ¨å¯ç”¨æ¨¡å‹åˆ—è¡¨ä¸­ï¼Œå°†ä½¿ç”¨é»˜è®¤æ¨¡å?${this.defaultModel}`,
      );
    }
  }

  /**
   * è·å–å½“å‰ä½¿ç”¨çš„æ¨¡å?
   * @returns å½“å‰æ¨¡å‹åç§°
   */
  public getModel(): string {
    return this.defaultModel;
  }

  /**
   * è·å–æ‰€æœ‰å¯ç”¨çš„æ¨¡å‹
   * @returns å¯ç”¨æ¨¡å‹åˆ—è¡¨
   */
  public getAvailableModels(): string[] {
    return [...this.availableModels];
  }

  async createChatCompletion(
    messages: ChatMessage[],
    options: ChatCompletionOptions = {},
  ): Promise<any> {
    try {
      // ä½¿ç”¨HttpClientè¿›è¡Œè¯·æ±‚ï¼Œè‡ªåŠ¨å¤„ç†é‡è¯•å’Œè¶…æ—¶
      return await this.httpClient.request(`${this.baseURL}/chat/completions`, {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
          "Authorization": `Bearer ${this.token}`,
        },
        body: JSON.stringify({
          model: options.model || this.defaultModel,
          messages,
          temperature: options.temperature ?? 0.7,
          top_p: options.top_p ?? 1,
          max_tokens: options.max_tokens ?? 2000,
          stream: options.stream ?? false,
          response_format: options.response_format,
        }),
        timeout: 60000, // 60ç§’è¶…æ—?
        retries: 3, // æœ€å¤šé‡è¯?æ¬?
        retryDelay: 1000, // é‡è¯•é—´éš”1ç§?
      });
    } catch (error) {
      throw new Error(`åˆ›å»ºèŠå¤©å®Œæˆå¤±è´¥: ${(error as Error).message}`);
    }
  }
}
